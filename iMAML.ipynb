{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchmeta.datasets.helpers import omniglot, miniimagenet\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "import higher\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    seed: int=0\n",
    "    dataset: str='omniglot'\n",
    "    hg_mode: str='CG'\n",
    "    no_cuda: bool=False    \n",
    "\n",
    "args = Args()\n",
    "\n",
    "log_interval = 100\n",
    "eval_interval = 500\n",
    "inner_log_interval = None\n",
    "ways = 5\n",
    "inner_log_interval_test = None\n",
    "batch_size = 16\n",
    "n_tasks_test = 1000  # usually 1000 tasks are used for testing\n",
    "\n",
    "\n",
    "reg_param = 2  # reg_param = 2\n",
    "T, K = 16, 5  # T, K = 16, 5\n",
    "\n",
    "T_test = T\n",
    "inner_lr = .1\n",
    "\n",
    "cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args(seed=0, dataset='omniglot', hg_mode='CG', no_cuda=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "# 5-ways 1-shot\n",
    "dataset = omniglot(\n",
    "    \"data\", ways=ways, shots=1, test_shots=15, meta_train=True, download=True)\n",
    "test_dataset = omniglot(\n",
    "    \"data\", ways=ways, shots=1, test_shots=15, meta_test=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (4): Flatten(start_dim=1, end_dim=-1)\n",
       "  (5): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_layer(ic, oc, ):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ic, oc, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.BatchNorm2d(oc, momentum=1., affine=True,\n",
    "                       track_running_stats=True # When this is true is called the \"transfuctive setting\"\n",
    "                       )\n",
    "    )\n",
    "\n",
    "meta_model = nn.Sequential(\n",
    "    conv_layer(1, 64),\n",
    "    conv_layer(64, 64),\n",
    "    conv_layer(64, 64),\n",
    "    conv_layer(64, 64),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64, 5) # hidden_size, ways\n",
    ")\n",
    "\n",
    "for m in meta_model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        #m.weight.data.normal_(0, 0.01)\n",
    "        #m.bias.data = torch.ones(m.bias.data.size())\n",
    "        m.weight.data.zero_()\n",
    "        m.bias.data.zero_()\n",
    "        \n",
    "meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = BatchMetaDataLoader(\n",
    "    dataset, batch_size=batch_size, **kwargs)\n",
    "test_dataloader = BatchMetaDataLoader(\n",
    "    test_dataset, batch_size=batch_size, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_opt = torch.optim.Adam(params=meta_model.parameters())\n",
    "outer_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 1, 28, 28])\n",
      "torch.Size([16, 5])\n",
      "\n",
      "torch.Size([16, 75, 1, 28, 28])\n",
      "torch.Size([16, 75])\n"
     ]
    }
   ],
   "source": [
    "for k, batch in enumerate(dataloader):\n",
    "    break\n",
    "    \n",
    "for i in batch['train']:\n",
    "    print(i.size())\n",
    "\n",
    "print()\n",
    "\n",
    "for i in batch['test']:\n",
    "    print(i.size())\n",
    "\n",
    "train_input  = tr_x = tr_xs = batch['train'][0][0]\n",
    "train_target = tr_y = tr_ys = batch['train'][1][0]\n",
    "test_input   = ts_x = ts_xs = batch['test'][0][0]\n",
    "test_target  = ts_y = ts_ys = batch['test'][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionalSequential(\n",
       "  (0): InnerFunctionalSequential(\n",
       "    (0): InnerFunctionalConv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): InnerFunctionalReLU(inplace=True)\n",
       "    (2): InnerFunctionalMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): InnerFunctionalBatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): InnerFunctionalSequential(\n",
       "    (0): InnerFunctionalConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): InnerFunctionalReLU(inplace=True)\n",
       "    (2): InnerFunctionalMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): InnerFunctionalBatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (2): InnerFunctionalSequential(\n",
       "    (0): InnerFunctionalConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): InnerFunctionalReLU(inplace=True)\n",
       "    (2): InnerFunctionalMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): InnerFunctionalBatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (3): InnerFunctionalSequential(\n",
       "    (0): InnerFunctionalConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): InnerFunctionalReLU(inplace=True)\n",
       "    (2): InnerFunctionalMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): InnerFunctionalBatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (4): InnerFunctionalFlatten(start_dim=1, end_dim=-1)\n",
       "  (5): InnerFunctionalLinear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel = higher.monkeypatch(meta_model, copy_initial_weights=True)\n",
    "fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = len(list(meta_model.parameters()))\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "val_loss = None\n",
    "val_acc = None\n",
    "\n",
    "inner_opt_kwargs = {'step_size': inner_lr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentiableOptimizer:\n",
    "    def __init__(self, loss_f, dim_mult, data_or_iter=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loss_f: callable with signature (params, hparams, [data optional]) -> loss tensor\n",
    "            data_or_iter: (x, y) or iterator over the data needed for loss_f\n",
    "        \"\"\"\n",
    "        self.data_iterator = None\n",
    "        if data_or_iter:\n",
    "            self.data_iterator = data_or_iter if hasattr(data_or_iter, '__next__') else repeat(data_or_iter)\n",
    "\n",
    "        self.loss_f = loss_f\n",
    "        self.dim_mult = dim_mult\n",
    "        self.curr_loss = None\n",
    "\n",
    "    def get_opt_params(self, params):\n",
    "        opt_params = [p for p in params]\n",
    "        opt_params.extend([torch.zeros_like(p) for p in params for _ in range(self.dim_mult-1) ])\n",
    "        return opt_params\n",
    "\n",
    "    def step(self, params, hparams, create_graph):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, params, hparams, create_graph=True):\n",
    "        with torch.enable_grad():\n",
    "            return self.step(params, hparams, create_graph)\n",
    "\n",
    "    def get_loss(self, params, hparams):\n",
    "        if self.data_iterator:\n",
    "            data = next(self.data_iterator)\n",
    "            self.curr_loss = self.loss_f(params, hparams, data)\n",
    "        else:\n",
    "            self.curr_loss = self.loss_f(params, hparams)\n",
    "        return self.curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(DifferentiableOptimizer):\n",
    "    def __init__(self, loss_f, step_size, data_or_iter=None):\n",
    "        super(GradientDescent, self).__init__(loss_f, dim_mult=1, data_or_iter=data_or_iter)\n",
    "        self.step_size_f = step_size if callable(step_size) else lambda x: step_size\n",
    "\n",
    "    def step(self, params, hparams, create_graph):\n",
    "        loss = self.get_loss(params, hparams)\n",
    "        sz = self.step_size_f(hparams)\n",
    "        return gd_step(params, loss, sz, create_graph=create_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_step(params, loss, step_size, create_graph=True):\n",
    "    grads = torch.autograd.grad(loss, params, create_graph=create_graph)\n",
    "    return [w - step_size * g for w, g in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_opt_class = GradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inner_opt(train_loss):\n",
    "    return inner_opt_class(train_loss, **inner_opt_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_reg_f(bias, params):\n",
    "    # L2 biasd regularization\n",
    "    return sum([((b-p)**2).sum() for b, p in zip(bias, params)])\n",
    "\n",
    "\n",
    "def train_loss_f(params, hparams):\n",
    "    out = fmodel(train_input, params=params)\n",
    "    return F.cross_entropy(out, train_target) + \\\n",
    "           0.5*reg_param*bias_reg_f(hparams, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GradientDescent at 0x23d9c661a20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner_opt = inner_opt_class(train_loss_f, **inner_opt_kwargs)\n",
    "inner_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single task inner loop\n",
    "params = [\n",
    "    p.detach().clone().requires_grad_(True)\n",
    "    for p in meta_model.parameters()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = meta_model.parameters()\n",
    "optim = inner_opt\n",
    "n_steps = T # 16\n",
    "log_interval = inner_log_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_history = [optim.get_opt_params(params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_interval is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0, Loss: 1.609438\n",
      "t=1, Loss: 0.603103\n",
      "t=2, Loss: 0.459852\n",
      "t=3, Loss: 0.516618\n",
      "t=4, Loss: 0.375373\n",
      "t=5, Loss: 0.125849\n",
      "t=6, Loss: 0.075950\n",
      "t=7, Loss: 0.061891\n",
      "t=8, Loss: 0.053470\n",
      "t=9, Loss: 0.047374\n",
      "t=10, Loss: 0.042667\n",
      "t=11, Loss: 0.038868\n",
      "t=12, Loss: 0.035731\n",
      "t=13, Loss: 0.033080\n",
      "t=14, Loss: 0.030811\n",
      "t=15, Loss: 0.028845\n"
     ]
    }
   ],
   "source": [
    "for t in range(n_steps):\n",
    "    params_history.append(\n",
    "        optim(params_history[-1], hparams, create_graph=False)\\\n",
    "    )\n",
    "    print('t={}, Loss: {:.6f}'.format(t, optim.curr_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_param  = params_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_param[10].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(fmodel.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_fp_map = GradientDescent(loss_f=train_loss_f, step_size=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the hypergradient by applying K steps of the\n",
    "# conjugate gradient method (CG).\n",
    "# It can end earlier when tol is reached\n",
    "\n",
    "params = [w.detach().requires_grad_(True) for w in last_param]\n",
    "hparams = list(meta_model.parameters())\n",
    "stochastic = False\n",
    "set_grad = True\n",
    "tol = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer_loss via task.val_loss_f\n",
    "out = fmodel(test_input, params=params)\n",
    "val_loss = F.cross_entropy(out, test_target) / batch_size\n",
    "o_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-7c61eb1b3f98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mgrad_outer_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_unused_zero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mgrad_outer_hparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_unused_zero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mfp_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcg_fp_map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-7c61eb1b3f98>\u001b[0m in \u001b[0;36mgrad_unused_zero\u001b[1;34m(output, inputs, grad_outputs, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m      9\u001b[0m     grads = torch.autograd.grad(\n\u001b[0;32m     10\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_unused\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         retain_graph=retain_graph, create_graph=create_graph)\n\u001b[0m\u001b[0;32m     12\u001b[0m     return tuple(\n\u001b[0;32m     13\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    202\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[0;32m    203\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         inputs, allow_unused)\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# get outer gradients\n",
    "def grad_unused_zero(\n",
    "    output,\n",
    "    inputs,\n",
    "    grad_outputs=None,\n",
    "    retain_graph=False,\n",
    "    create_graph=False,\n",
    "):\n",
    "    grads = torch.autograd.grad(\n",
    "        output, inputs, grad_outputs, allow_unused=True,\n",
    "        retain_graph=retain_graph, create_graph=create_graph)\n",
    "    return tuple(\n",
    "        torch.zeros_like(v) if g is None else g\n",
    "        for g, v in zip(grads, inputs)\n",
    "    )\n",
    "\n",
    "grad_outer_w = grad_unused_zero(o_loss, params)\n",
    "grad_outer_hparams = grad_unused_zero(o_loss, hparams)\n",
    "\n",
    "fp_map = cg_fp_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-d4c466282498>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# outer_loss via task.val_loss_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mo_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\higher\\patch.py\u001b[0m in \u001b[0;36m_patched_forward\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_refill_params_box\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;31m# Clean up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\higher\\patch.py\u001b[0m in \u001b[0;36mpatched_forward\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    385\u001b[0m                 \u001b[0m_warnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUserWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrue_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMonkeyPatched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"forward\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatched_forward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def dfp_map_dw(xs):\n",
    "    if stochastic:\n",
    "        w_mapped_in = fp_map(params, hparams)\n",
    "        Jfp_mapTv = torch.autograd.grad(w_mapped_in, params, grad_outputs=xs, retain_graph=False)\n",
    "    else:\n",
    "        Jfp_mapTv = torch.autograd.grad(w_mapped, params, grad_outputs=xs, retain_graph=True)\n",
    "    return [v - j for v, j in zip(xs, Jfp_mapTv)]\n",
    "\n",
    "# Conjugate Gradient\n",
    "Ax = dfp_map_dw\n",
    "b = grad_outer_w\n",
    "max_iter = 100\n",
    "epsilon = 1.0e-5\n",
    "\n",
    "x_last = [torch.zeros_like(bb) for bb in b]\n",
    "r_last = [torch.zeros_like(bb).copy_(bb) for bb in b]\n",
    "p_last = [torch.zeros_like(rr).copy_(rr) for rr in r_last]\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    Ap = Ax(p_last)\n",
    "    Ap_vec = torch.cat([xx.view(-1) for xx in Ap])\n",
    "    p_last_vec = torch.cat([xx.view(-1) for xx in p_last])\n",
    "    r_last_vec = torch.cat([xx.view(-1) for xx in r_last])\n",
    "    rTr = torch.sum(r_last_vec * r_last_vec)\n",
    "    pAp = torch.sum(p_last_vec * Ap_vec)\n",
    "    alpha = rTr / pAp\n",
    "    \n",
    "    x = [xx + alpha * pp for xx, pp in zip(x_last, p_last)]    \n",
    "    r = [rr - alpha * pp for rr, pp in zip(r_last, Ap)]\n",
    "    r_vec = torch.cat([xx.view(-1) for xx in r_last])\n",
    "    \n",
    "    if float(torch.norm(r_vec)) < epsilon:\n",
    "        break\n",
    "        \n",
    "    beta = torch.sum(r_vec * r_vec) / rTr\n",
    "    p = [rr + beta * pp for rr, pp in zip(r, p_last)]\n",
    "    \n",
    "    x_last = x\n",
    "    p_last = p\n",
    "    r_last = r\n",
    "    \n",
    "vs = x_last\n",
    "\n",
    "if stochastic:\n",
    "    w_mapped = fp_map(params, hparams)\n",
    "    \n",
    "grads = torch.autograd.grad(w_mapped, hparams, grad_outputs=vs)\n",
    "grads = [g + v for g, v in zip(grads, grad_outer_hparams)]\n",
    "\n",
    "if set_grad:\n",
    "    for l, g in zip(hparams, grads):\n",
    "        if l.grad is None:\n",
    "            l.grad = torch.zeros_like(l)\n",
    "        if g is not None:\n",
    "            l.grad += g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg.CG(\n",
    "    last_param, \n",
    "    list(meta_model.parameters()), \n",
    "    K=K, \n",
    "    fp_map=cg_fp_map, \n",
    "    outer_loss=task.val_loss_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
