{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchmeta.datasets.helpers import omniglot, miniimagenet\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "import higher\n",
    "\n",
    "import hypergrad as hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args(seed=0, dataset='omniglot', hg_mode='CG', no_cuda=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    seed: int=0\n",
    "    dataset: str='omniglot'\n",
    "    hg_mode: str='CG'\n",
    "    no_cuda: bool=False    \n",
    "\n",
    "args = Args()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 100\n",
    "eval_interval = 500\n",
    "inner_log_interval = None\n",
    "ways = 5\n",
    "inner_log_interval_test = None\n",
    "batch_size = 16\n",
    "n_tasks_test = 1000  # usually 1000 tasks are used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_param = 2  # reg_param = 2\n",
    "T, K = 16, 5  # T, K = 16, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = T\n",
    "inner_lr = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 1, 'pin_memory': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following are for reproducibility on GPU,\n",
    "# see https://pytorch.org/docs/master/notes/randomness.html\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.random.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-ways 1-shot\n",
    "dataset = omniglot(\n",
    "    \"data\", ways=ways, shots=1, test_shots=15, meta_train=True, download=True)\n",
    "test_dataset = omniglot(\n",
    "    \"data\", ways=ways, shots=1, test_shots=15, meta_test=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(ic, oc, ):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ic, oc, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.BatchNorm2d(oc, momentum=1., affine=True,\n",
    "                       track_running_stats=True # When this is true is called the \"transfuctive setting\"\n",
    "                       )\n",
    "    )\n",
    "\n",
    "meta_model = nn.Sequential(\n",
    "    conv_layer(1, 64),\n",
    "    conv_layer(64, 64),\n",
    "    conv_layer(64, 64),\n",
    "    conv_layer(64, 64),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64, 5) # hidden_size, ways\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (4): Flatten(start_dim=1, end_dim=-1)\n",
       "  (5): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(meta_model.parameters())[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights properly\n",
    "for m in meta_model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        #m.weight.data.normal_(0, 0.01)\n",
    "        #m.bias.data = torch.ones(m.bias.data.size())\n",
    "        m.weight.data.zero_()\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = BatchMetaDataLoader(dataset, batch_size=batch_size, **kwargs)\n",
    "test_dataloader = BatchMetaDataLoader(test_dataset, batch_size=batch_size, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_opt = torch.optim.Adam(params=meta_model.parameters())\n",
    "outer_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_opt_class = hg.GradientDescent\n",
    "inner_opt_kwargs = {'step_size': inner_lr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inner_opt(train_loss):\n",
    "    return inner_opt_class(train_loss, **inner_opt_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    \"\"\"\n",
    "    Handles the train and valdation loss for a single task\n",
    "    \"\"\"\n",
    "    def __init__(self, reg_param, meta_model, data, batch_size=None):\n",
    "        device = next(meta_model.parameters()).device\n",
    "\n",
    "        # stateless version of meta_model\n",
    "        self.fmodel = higher.monkeypatch(meta_model, device=device, copy_initial_weights=True)\n",
    "\n",
    "        self.n_params = len(list(meta_model.parameters()))\n",
    "        self.train_input, self.train_target, self.test_input, self.test_target = data\n",
    "        self.reg_param = reg_param\n",
    "        self.batch_size = 1 if not batch_size else batch_size\n",
    "        self.val_loss, self.val_acc = None, None\n",
    "\n",
    "    def bias_reg_f(self, bias, params):\n",
    "        # l2 biased regularization\n",
    "        return sum([((b - p) ** 2).sum() for b, p in zip(bias, params)])\n",
    "\n",
    "    def train_loss_f(self, params, hparams):\n",
    "        # biased regularized cross-entropy loss where the bias are the meta-parameters in hparams\n",
    "        out = self.fmodel(self.train_input, params=params)\n",
    "        return F.cross_entropy(out, self.train_target) + 0.5 * self.reg_param * self.bias_reg_f(hparams, params)\n",
    "\n",
    "    def val_loss_f(self, params, hparams):\n",
    "        # cross-entropy loss (uses only the task-specific weights in params\n",
    "        out = self.fmodel(self.test_input, params=params)\n",
    "        val_loss = F.cross_entropy(out, self.test_target)/self.batch_size\n",
    "        self.val_loss = val_loss.item()  # avoid memory leaks\n",
    "\n",
    "        pred = out.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        self.val_acc = pred.eq(self.test_target.view_as(pred)).sum().item() / len(self.test_target)\n",
    "\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_loop(hparams, params, optim, n_steps, log_interval, create_graph=False):\n",
    "    params_history = [optim.get_opt_params(params)]\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        params_history.append(optim(params_history[-1], hparams, create_graph=create_graph))\n",
    "\n",
    "        if log_interval and (t % log_interval == 0 or t == n_steps-1):\n",
    "            print('t={}, Loss: {:.6f}'.format(t, optim.curr_loss.item()))\n",
    "\n",
    "    return params_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(n_tasks, dataloader, meta_model, n_steps, get_inner_opt, reg_param, log_interval=None):\n",
    "    meta_model.train()\n",
    "    device = next(meta_model.parameters()).device\n",
    "\n",
    "    val_losses, val_accs = [], []\n",
    "    for k, batch in enumerate(dataloader):\n",
    "        tr_xs, tr_ys = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "        tst_xs, tst_ys = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "        for t_idx, (tr_x, tr_y, tst_x, tst_y) in enumerate(zip(tr_xs, tr_ys, tst_xs, tst_ys)):\n",
    "            task = Task(reg_param, meta_model, (tr_x, tr_y, tst_x, tst_y))\n",
    "            inner_opt = get_inner_opt(task.train_loss_f)\n",
    "\n",
    "            params = [p.detach().clone().requires_grad_(True) for p in meta_model.parameters()]\n",
    "            last_param = inner_loop(meta_model.parameters(), params, inner_opt, n_steps, log_interval=log_interval)[-1]\n",
    "\n",
    "            task.val_loss_f(last_param, meta_model.parameters())\n",
    "\n",
    "            val_losses.append(task.val_loss)\n",
    "            val_accs.append(task.val_acc)\n",
    "\n",
    "            if len(val_accs) >= n_tasks:\n",
    "                return np.array(val_losses), np.array(val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT k=0 (4.441s F: 2.535s, B: 1.900s) Val Loss: 8.89e-01, Val Acc: 69.58.\n",
      "Test loss 9.38e-01 +- 1.92e-01: Test acc: 67.63 +- 1.09e+01 (mean +- std over 1000 tasks).\n",
      "MT k=100 (3.357s F: 1.280s, B: 2.073s) Val Loss: 2.96e-01, Val Acc: 95.33.\n",
      "MT k=200 (3.216s F: 1.230s, B: 1.981s) Val Loss: 2.16e-01, Val Acc: 97.75.\n",
      "MT k=300 (3.248s F: 1.230s, B: 2.015s) Val Loss: 1.73e-01, Val Acc: 98.42.\n",
      "MT k=400 (3.196s F: 1.216s, B: 1.975s) Val Loss: 1.23e-01, Val Acc: 99.17.\n",
      "MT k=500 (3.248s F: 1.231s, B: 2.013s) Val Loss: 1.60e-01, Val Acc: 98.08.\n",
      "Test loss 2.01e-01 +- 9.18e-02: Test acc: 97.10 +- 3.93e+00 (mean +- std over 1000 tasks).\n",
      "MT k=600 (3.229s F: 1.224s, B: 2.001s) Val Loss: 1.53e-01, Val Acc: 97.42.\n",
      "MT k=700 (3.236s F: 1.243s, B: 1.990s) Val Loss: 1.46e-01, Val Acc: 98.00.\n",
      "MT k=800 (3.518s F: 1.277s, B: 2.236s) Val Loss: 1.01e-01, Val Acc: 97.42.\n",
      "MT k=900 (3.613s F: 1.304s, B: 2.303s) Val Loss: 1.75e-01, Val Acc: 95.42.\n",
      "MT k=1000 (3.575s F: 1.281s, B: 2.290s) Val Loss: 1.24e-01, Val Acc: 96.42.\n",
      "Test loss 1.78e-01 +- 1.34e-01: Test acc: 94.43 +- 5.40e+00 (mean +- std over 1000 tasks).\n",
      "MT k=1100 (3.508s F: 1.248s, B: 2.256s) Val Loss: 1.19e-01, Val Acc: 96.17.\n",
      "MT k=1200 (3.467s F: 1.238s, B: 2.223s) Val Loss: 1.16e-01, Val Acc: 96.25.\n",
      "MT k=1300 (3.406s F: 1.218s, B: 2.183s) Val Loss: 9.40e-02, Val Acc: 97.00.\n",
      "MT k=1400 (3.471s F: 1.278s, B: 2.187s) Val Loss: 1.12e-01, Val Acc: 95.75.\n",
      "MT k=1500 (3.525s F: 1.261s, B: 2.260s) Val Loss: 1.06e-01, Val Acc: 96.83.\n",
      "Test loss 1.56e-01 +- 1.38e-01: Test acc: 94.97 +- 4.90e+00 (mean +- std over 1000 tasks).\n",
      "MT k=1600 (3.418s F: 1.226s, B: 2.187s) Val Loss: 1.44e-01, Val Acc: 95.67.\n",
      "MT k=1700 (3.286s F: 1.221s, B: 2.061s) Val Loss: 7.61e-02, Val Acc: 97.25.\n",
      "MT k=1800 (3.513s F: 1.300s, B: 2.209s) Val Loss: 5.07e-02, Val Acc: 98.75.\n",
      "MT k=1900 (3.513s F: 1.290s, B: 2.217s) Val Loss: 8.99e-02, Val Acc: 96.92.\n",
      "MT k=2000 (3.550s F: 1.267s, B: 2.278s) Val Loss: 9.41e-02, Val Acc: 96.75.\n",
      "Test loss 1.42e-01 +- 1.37e-01: Test acc: 95.27 +- 4.81e+00 (mean +- std over 1000 tasks).\n",
      "MT k=2100 (3.357s F: 1.271s, B: 2.082s) Val Loss: 8.48e-02, Val Acc: 96.92.\n",
      "MT k=2200 (3.527s F: 1.284s, B: 2.237s) Val Loss: 1.09e-01, Val Acc: 96.00.\n",
      "MT k=2300 (3.565s F: 1.284s, B: 2.275s) Val Loss: 8.18e-02, Val Acc: 97.00.\n",
      "MT k=2400 (3.482s F: 1.270s, B: 2.207s) Val Loss: 1.19e-01, Val Acc: 96.25.\n",
      "MT k=2500 (3.541s F: 1.282s, B: 2.254s) Val Loss: 1.01e-01, Val Acc: 96.75.\n",
      "Test loss 1.43e-01 +- 1.37e-01: Test acc: 95.23 +- 4.79e+00 (mean +- std over 1000 tasks).\n",
      "MT k=2600 (3.492s F: 1.253s, B: 2.232s) Val Loss: 6.52e-02, Val Acc: 98.33.\n",
      "MT k=2700 (3.428s F: 1.275s, B: 2.148s) Val Loss: 5.14e-02, Val Acc: 98.33.\n",
      "MT k=2800 (3.415s F: 1.276s, B: 2.135s) Val Loss: 1.02e-01, Val Acc: 96.67.\n",
      "MT k=2900 (3.441s F: 1.246s, B: 2.191s) Val Loss: 8.38e-02, Val Acc: 97.58.\n",
      "MT k=3000 (3.641s F: 1.313s, B: 2.322s) Val Loss: 5.74e-02, Val Acc: 98.25.\n",
      "Test loss 1.40e-01 +- 1.39e-01: Test acc: 95.38 +- 4.72e+00 (mean +- std over 1000 tasks).\n",
      "MT k=3100 (8.963s F: 1.466s, B: 7.470s) Val Loss: 5.52e-02, Val Acc: 98.25.\n",
      "MT k=3200 (3.768s F: 1.254s, B: 2.507s) Val Loss: 8.41e-02, Val Acc: 96.83.\n",
      "MT k=3300 (3.789s F: 1.255s, B: 2.528s) Val Loss: 8.88e-02, Val Acc: 97.25.\n",
      "MT k=3400 (3.703s F: 1.244s, B: 2.451s) Val Loss: 6.17e-02, Val Acc: 98.08.\n",
      "MT k=3500 (3.669s F: 1.257s, B: 2.406s) Val Loss: 6.79e-02, Val Acc: 97.67.\n",
      "Test loss 1.33e-01 +- 1.38e-01: Test acc: 95.60 +- 4.79e+00 (mean +- std over 1000 tasks).\n",
      "MT k=3600 (3.776s F: 1.266s, B: 2.504s) Val Loss: 8.58e-02, Val Acc: 97.17.\n",
      "MT k=3700 (3.645s F: 1.247s, B: 2.392s) Val Loss: 5.08e-02, Val Acc: 98.42.\n",
      "MT k=3800 (3.301s F: 1.224s, B: 2.070s) Val Loss: 1.05e-01, Val Acc: 95.33.\n",
      "MT k=3900 (3.462s F: 1.267s, B: 2.188s) Val Loss: 4.38e-02, Val Acc: 98.67.\n",
      "MT k=4000 (3.529s F: 1.268s, B: 2.255s) Val Loss: 8.73e-02, Val Acc: 96.83.\n",
      "Test loss 1.31e-01 +- 1.37e-01: Test acc: 95.66 +- 4.58e+00 (mean +- std over 1000 tasks).\n",
      "MT k=4100 (3.514s F: 1.297s, B: 2.212s) Val Loss: 7.01e-02, Val Acc: 98.17.\n",
      "MT k=4200 (3.380s F: 1.258s, B: 2.118s) Val Loss: 7.94e-02, Val Acc: 97.50.\n",
      "MT k=4300 (3.500s F: 1.259s, B: 2.236s) Val Loss: 8.39e-02, Val Acc: 97.50.\n",
      "MT k=4400 (3.415s F: 1.230s, B: 2.180s) Val Loss: 9.11e-02, Val Acc: 95.83.\n",
      "MT k=4500 (3.679s F: 1.263s, B: 2.411s) Val Loss: 4.54e-02, Val Acc: 98.50.\n",
      "Test loss 1.36e-01 +- 1.40e-01: Test acc: 95.53 +- 4.69e+00 (mean +- std over 1000 tasks).\n",
      "MT k=4600 (3.422s F: 1.233s, B: 2.184s) Val Loss: 8.90e-02, Val Acc: 97.00.\n",
      "MT k=4700 (3.381s F: 1.264s, B: 2.112s) Val Loss: 1.04e-01, Val Acc: 96.08.\n",
      "MT k=4800 (3.490s F: 1.238s, B: 2.248s) Val Loss: 1.31e-01, Val Acc: 95.42.\n",
      "MT k=4900 (3.503s F: 1.246s, B: 2.253s) Val Loss: 7.61e-02, Val Acc: 97.50.\n",
      "MT k=5000 (3.731s F: 1.323s, B: 2.404s) Val Loss: 5.83e-02, Val Acc: 97.75.\n",
      "Test loss 1.19e-01 +- 1.44e-01: Test acc: 96.14 +- 4.59e+00 (mean +- std over 1000 tasks).\n",
      "MT k=5100 (3.309s F: 1.236s, B: 2.070s) Val Loss: 9.65e-02, Val Acc: 97.33.\n",
      "MT k=5200 (3.543s F: 1.278s, B: 2.260s) Val Loss: 7.68e-02, Val Acc: 97.92.\n",
      "MT k=5300 (3.626s F: 1.306s, B: 2.315s) Val Loss: 6.10e-02, Val Acc: 97.75.\n",
      "MT k=5400 (3.382s F: 1.302s, B: 2.075s) Val Loss: 4.43e-02, Val Acc: 98.92.\n",
      "MT k=5500 (3.679s F: 1.323s, B: 2.352s) Val Loss: 2.62e-02, Val Acc: 99.33.\n",
      "Test loss 1.21e-01 +- 1.29e-01: Test acc: 95.98 +- 4.37e+00 (mean +- std over 1000 tasks).\n",
      "MT k=5600 (3.585s F: 1.306s, B: 2.272s) Val Loss: 4.45e-02, Val Acc: 98.83.\n",
      "MT k=5700 (3.561s F: 1.306s, B: 2.249s) Val Loss: 7.31e-02, Val Acc: 97.75.\n",
      "MT k=5800 (3.515s F: 1.269s, B: 2.240s) Val Loss: 6.37e-02, Val Acc: 97.67.\n",
      "MT k=5900 (3.410s F: 1.260s, B: 2.143s) Val Loss: 4.50e-02, Val Acc: 99.00.\n",
      "MT k=6000 (3.530s F: 1.264s, B: 2.260s) Val Loss: 7.19e-02, Val Acc: 97.67.\n",
      "Test loss 1.31e-01 +- 1.51e-01: Test acc: 95.83 +- 4.66e+00 (mean +- std over 1000 tasks).\n",
      "MT k=6100 (3.382s F: 1.253s, B: 2.123s) Val Loss: 8.41e-02, Val Acc: 97.17.\n",
      "MT k=6200 (3.450s F: 1.245s, B: 2.201s) Val Loss: 5.26e-02, Val Acc: 97.92.\n",
      "MT k=6300 (3.467s F: 1.266s, B: 2.197s) Val Loss: 9.31e-02, Val Acc: 97.08.\n",
      "MT k=6400 (3.480s F: 1.257s, B: 2.218s) Val Loss: 8.99e-02, Val Acc: 97.08.\n",
      "MT k=6500 (3.327s F: 1.243s, B: 2.080s) Val Loss: 7.45e-02, Val Acc: 98.00.\n",
      "Test loss 1.19e-01 +- 1.51e-01: Test acc: 96.24 +- 4.38e+00 (mean +- std over 1000 tasks).\n",
      "MT k=6600 (3.492s F: 1.256s, B: 2.231s) Val Loss: 3.29e-02, Val Acc: 99.08.\n",
      "MT k=6700 (3.391s F: 1.245s, B: 2.142s) Val Loss: 4.44e-02, Val Acc: 98.67.\n",
      "MT k=6800 (3.524s F: 1.267s, B: 2.252s) Val Loss: 5.16e-02, Val Acc: 98.00.\n",
      "MT k=6900 (3.455s F: 1.224s, B: 2.225s) Val Loss: 8.63e-02, Val Acc: 97.00.\n",
      "MT k=7000 (3.339s F: 1.287s, B: 2.036s) Val Loss: 3.73e-02, Val Acc: 99.00.\n",
      "Test loss 1.14e-01 +- 1.30e-01: Test acc: 96.26 +- 4.19e+00 (mean +- std over 1000 tasks).\n",
      "MT k=7100 (3.608s F: 1.300s, B: 2.302s) Val Loss: 1.30e-01, Val Acc: 95.67.\n",
      "MT k=7200 (3.669s F: 1.315s, B: 2.348s) Val Loss: 1.05e-01, Val Acc: 96.08.\n",
      "MT k=7300 (3.578s F: 1.251s, B: 2.319s) Val Loss: 7.31e-02, Val Acc: 97.08.\n",
      "MT k=7400 (3.495s F: 1.258s, B: 2.232s) Val Loss: 7.21e-02, Val Acc: 97.67.\n",
      "MT k=7500 (3.503s F: 1.275s, B: 2.222s) Val Loss: 8.80e-02, Val Acc: 97.33.\n",
      "Test loss 1.24e-01 +- 1.51e-01: Test acc: 95.94 +- 4.81e+00 (mean +- std over 1000 tasks).\n",
      "MT k=7600 (3.245s F: 1.262s, B: 1.977s) Val Loss: 6.17e-02, Val Acc: 98.00.\n",
      "MT k=7700 (3.347s F: 1.220s, B: 2.121s) Val Loss: 4.06e-02, Val Acc: 98.92.\n",
      "MT k=7800 (3.522s F: 1.217s, B: 2.298s) Val Loss: 1.16e-01, Val Acc: 96.25.\n",
      "MT k=7900 (3.560s F: 1.277s, B: 2.277s) Val Loss: 1.30e-01, Val Acc: 96.08.\n",
      "MT k=8000 (3.513s F: 1.237s, B: 2.271s) Val Loss: 4.13e-02, Val Acc: 98.33.\n",
      "Test loss 1.24e-01 +- 1.61e-01: Test acc: 95.98 +- 4.71e+00 (mean +- std over 1000 tasks).\n",
      "MT k=8100 (3.476s F: 1.217s, B: 2.254s) Val Loss: 5.86e-02, Val Acc: 98.33.\n",
      "MT k=8200 (3.321s F: 1.212s, B: 2.105s) Val Loss: 5.35e-02, Val Acc: 98.00.\n",
      "MT k=8300 (3.375s F: 1.267s, B: 2.103s) Val Loss: 7.98e-02, Val Acc: 97.33.\n",
      "MT k=8400 (3.507s F: 1.302s, B: 2.200s) Val Loss: 3.01e-02, Val Acc: 99.25.\n",
      "MT k=8500 (3.491s F: 1.316s, B: 2.171s) Val Loss: 7.26e-02, Val Acc: 97.50.\n",
      "Test loss 1.32e-01 +- 1.50e-01: Test acc: 95.79 +- 4.48e+00 (mean +- std over 1000 tasks).\n",
      "MT k=8600 (3.486s F: 1.228s, B: 2.253s) Val Loss: 6.87e-02, Val Acc: 97.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT k=8700 (3.407s F: 1.234s, B: 2.158s) Val Loss: 5.52e-02, Val Acc: 98.50.\n",
      "MT k=8800 (3.486s F: 1.285s, B: 2.195s) Val Loss: 3.17e-02, Val Acc: 99.00.\n",
      "MT k=8900 (3.503s F: 1.282s, B: 2.216s) Val Loss: 9.59e-02, Val Acc: 97.92.\n",
      "MT k=9000 (3.253s F: 1.277s, B: 1.969s) Val Loss: 4.36e-02, Val Acc: 98.92.\n",
      "Test loss 1.18e-01 +- 1.35e-01: Test acc: 96.24 +- 4.24e+00 (mean +- std over 1000 tasks).\n",
      "MT k=9100 (3.438s F: 1.284s, B: 2.149s) Val Loss: 4.67e-02, Val Acc: 98.33.\n",
      "MT k=9200 (3.475s F: 1.348s, B: 2.120s) Val Loss: 6.06e-02, Val Acc: 97.50.\n"
     ]
    }
   ],
   "source": [
    "for k, batch in enumerate(dataloader):\n",
    "    start_time = time.time()\n",
    "    meta_model.train()\n",
    "\n",
    "    tr_xs, tr_ys = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    tst_xs, tst_ys = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "    outer_opt.zero_grad()\n",
    "\n",
    "    val_loss, val_acc = 0, 0\n",
    "    forward_time, backward_time = 0, 0\n",
    "    for t_idx, (tr_x, tr_y, tst_x, tst_y) in enumerate(\n",
    "            zip(tr_xs, tr_ys, tst_xs, tst_ys)\n",
    "    ):\n",
    "        start_time_task = time.time()\n",
    "\n",
    "        # single task set up\n",
    "        task = Task(\n",
    "            reg_param, meta_model, (tr_x, tr_y, tst_x, tst_y), \n",
    "            batch_size=tr_xs.shape[0]\n",
    "        )\n",
    "        inner_opt = get_inner_opt(task.train_loss_f)\n",
    "\n",
    "        # single task inner loop\n",
    "        params = [\n",
    "            p.detach().clone().requires_grad_(True) \n",
    "            for p in meta_model.parameters()\n",
    "        ]\n",
    "        last_param = inner_loop(\n",
    "            meta_model.parameters(), params, inner_opt, T, \n",
    "            log_interval=inner_log_interval)[-1]\n",
    "        forward_time_task = time.time() - start_time_task\n",
    "\n",
    "        # single task hypergradient computation\n",
    "        if args.hg_mode == 'CG':\n",
    "            # This is the approximation used in the paper CG stands for conjugate gradient\n",
    "            cg_fp_map = hg.GradientDescent(loss_f=task.train_loss_f, step_size=1.)\n",
    "            hg.CG(last_param, list(meta_model.parameters()), K=K, fp_map=cg_fp_map, outer_loss=task.val_loss_f)\n",
    "        elif args.hg_mode == 'fixed_point':\n",
    "            hg.fixed_point(last_param, list(meta_model.parameters()), K=K, fp_map=inner_opt,\n",
    "                           outer_loss=task.val_loss_f)\n",
    "\n",
    "        backward_time_task = time.time() - start_time_task - forward_time_task\n",
    "\n",
    "        val_loss += task.val_loss\n",
    "        val_acc += task.val_acc/task.batch_size\n",
    "\n",
    "        forward_time += forward_time_task\n",
    "        backward_time += backward_time_task\n",
    "\n",
    "    outer_opt.step()\n",
    "    step_time = time.time() - start_time\n",
    "\n",
    "    if k % log_interval == 0:\n",
    "        print('MT k={} ({:.3f}s F: {:.3f}s, B: {:.3f}s) Val Loss: {:.2e}, Val Acc: {:.2f}.'\n",
    "              .format(k, step_time, forward_time, backward_time, val_loss, 100. * val_acc))\n",
    "\n",
    "    if k % eval_interval == 0:\n",
    "        test_losses, test_accs = evaluate(n_tasks_test, test_dataloader, meta_model, T_test, get_inner_opt,\n",
    "                                      reg_param, log_interval=inner_log_interval_test)\n",
    "\n",
    "        print(\"Test loss {:.2e} +- {:.2e}: Test acc: {:.2f} +- {:.2e} (mean +- std over {} tasks).\"\n",
    "              .format(test_losses.mean(), test_losses.std(), 100. * test_accs.mean(),\n",
    "                      100.*test_accs.std(), len(test_losses)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
