{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchmeta.datasets.helpers import omniglot, miniimagenet\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "import higher\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    seed: int=0\n",
    "    dataset: str='omniglot'\n",
    "    hg_mode: str='CG'\n",
    "    no_cuda: bool=False    \n",
    "\n",
    "args = Args()\n",
    "\n",
    "log_interval = 100\n",
    "eval_interval = 500\n",
    "inner_log_interval = None\n",
    "ways = 5\n",
    "inner_log_interval_test = None\n",
    "batch_size = 16\n",
    "n_tasks_test = 1000  # usually 1000 tasks are used for testing\n",
    "\n",
    "\n",
    "reg_param = 2  # reg_param = 2\n",
    "T, K = 16, 5  # T, K = 16, 5\n",
    "\n",
    "T_test = T\n",
    "inner_lr = .1\n",
    "\n",
    "cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args(seed=0, dataset='omniglot', hg_mode='CG', no_cuda=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following are for reproducibility on GPU,\n",
    "# see https://pytorch.org/docs/master/notes/randomness.html\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.random.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-ways 1-shot\n",
    "dataset = omniglot(\n",
    "    \"data\", ways=ways, shots=1, test_shots=15, meta_train=True, download=True)\n",
    "test_dataset = omniglot(\n",
    "    \"data\", ways=ways, shots=1, test_shots=15, meta_test=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (4): Flatten(start_dim=1, end_dim=-1)\n",
       "  (5): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_layer(ic, oc, ):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ic, oc, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.BatchNorm2d(oc, momentum=1., affine=True,\n",
    "                       track_running_stats=True # When this is true is called the \"transfuctive setting\"\n",
    "                       )\n",
    "    )\n",
    "\n",
    "meta_model = nn.Sequential(\n",
    "    conv_layer(1, 64),\n",
    "    conv_layer(64, 64),\n",
    "    conv_layer(64, 64),\n",
    "    conv_layer(64, 64),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64, 5) # hidden_size, ways\n",
    ")\n",
    "\n",
    "for m in meta_model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        #m.weight.data.normal_(0, 0.01)\n",
    "        #m.bias.data = torch.ones(m.bias.data.size())\n",
    "        m.weight.data.zero_()\n",
    "        m.bias.data.zero_()\n",
    "        \n",
    "meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = BatchMetaDataLoader(\n",
    "    dataset, batch_size=batch_size, **kwargs)\n",
    "test_dataloader = BatchMetaDataLoader(\n",
    "    test_dataset, batch_size=batch_size, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_opt = torch.optim.Adam(params=meta_model.parameters())\n",
    "outer_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 1, 28, 28])\n",
      "torch.Size([16, 5])\n",
      "\n",
      "torch.Size([16, 75, 1, 28, 28])\n",
      "torch.Size([16, 75])\n"
     ]
    }
   ],
   "source": [
    "for k, batch in enumerate(dataloader):\n",
    "    break\n",
    "    \n",
    "for i in batch['train']:\n",
    "    print(i.size())\n",
    "\n",
    "print()\n",
    "\n",
    "for i in batch['test']:\n",
    "    print(i.size())\n",
    "\n",
    "train_input  = tr_x = tr_xs = batch['train'][0][0]\n",
    "train_target = tr_y = tr_ys = batch['train'][1][0]\n",
    "test_input   = ts_x = ts_xs = batch['test'][0][0]\n",
    "test_target  = ts_y = ts_ys = batch['test'][1][0]\n",
    "\n",
    "outer_opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionalSequential(\n",
       "  (0): InnerFunctionalSequential(\n",
       "    (0): InnerFunctionalConv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): InnerFunctionalReLU(inplace=True)\n",
       "    (2): InnerFunctionalMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): InnerFunctionalBatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): InnerFunctionalSequential(\n",
       "    (0): InnerFunctionalConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): InnerFunctionalReLU(inplace=True)\n",
       "    (2): InnerFunctionalMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): InnerFunctionalBatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (2): InnerFunctionalSequential(\n",
       "    (0): InnerFunctionalConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): InnerFunctionalReLU(inplace=True)\n",
       "    (2): InnerFunctionalMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): InnerFunctionalBatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (3): InnerFunctionalSequential(\n",
       "    (0): InnerFunctionalConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): InnerFunctionalReLU(inplace=True)\n",
       "    (2): InnerFunctionalMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): InnerFunctionalBatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (4): InnerFunctionalFlatten(start_dim=1, end_dim=-1)\n",
       "  (5): InnerFunctionalLinear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://higher.readthedocs.io/en/latest/patch.html\n",
    "# This functions produces a monkey-patched version of a module,\n",
    "# and a copy of its parameters for use as fast weights.\n",
    "fmodel = higher.monkeypatch(meta_model, copy_initial_weights=True)\n",
    "fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(meta_model.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(fmodel.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = len(list(meta_model.parameters()))\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "val_loss = None\n",
    "val_acc = None\n",
    "\n",
    "inner_opt_kwargs = {'step_size': inner_lr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentiableOptimizer:\n",
    "    def __init__(self, loss_f, dim_mult, data_or_iter=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loss_f: callable with signature (params, hparams, [data optional]) -> loss tensor\n",
    "            data_or_iter: (x, y) or iterator over the data needed for loss_f\n",
    "        \"\"\"\n",
    "        self.data_iterator = None\n",
    "        if data_or_iter:\n",
    "            self.data_iterator = data_or_iter if hasattr(data_or_iter, '__next__') else repeat(data_or_iter)\n",
    "\n",
    "        self.loss_f = loss_f\n",
    "        self.dim_mult = dim_mult\n",
    "        self.curr_loss = None\n",
    "\n",
    "    def get_opt_params(self, params):\n",
    "        opt_params = [p for p in params]\n",
    "        opt_params.extend([torch.zeros_like(p) for p in params for _ in range(self.dim_mult-1) ])\n",
    "        return opt_params\n",
    "\n",
    "    def step(self, params, hparams, create_graph):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, params, hparams, create_graph=True):\n",
    "        with torch.enable_grad():\n",
    "            return self.step(params, hparams, create_graph)\n",
    "\n",
    "    def get_loss(self, params, hparams):\n",
    "        if self.data_iterator:\n",
    "            data = next(self.data_iterator)\n",
    "            self.curr_loss = self.loss_f(params, hparams, data)\n",
    "        else:\n",
    "            self.curr_loss = self.loss_f(params, hparams)\n",
    "        return self.curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(DifferentiableOptimizer):\n",
    "    def __init__(self, loss_f, step_size, data_or_iter=None):\n",
    "        super(GradientDescent, self).__init__(loss_f, dim_mult=1, data_or_iter=data_or_iter)\n",
    "        self.step_size_f = step_size if callable(step_size) else lambda x: step_size\n",
    "\n",
    "    def step(self, params, hparams, create_graph):\n",
    "        loss = self.get_loss(params, hparams)\n",
    "        sz = self.step_size_f(hparams)\n",
    "        # partial derivative per task-specific parameter phi\n",
    "        return gd_step(params, loss, sz, create_graph=create_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_step(params, loss, step_size, create_graph=True):\n",
    "    grads = torch.autograd.grad(loss, params, create_graph=create_graph)\n",
    "    return [w - step_size * g for w, g in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_opt_class = GradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inner_opt(train_loss):\n",
    "    return inner_opt_class(train_loss, **inner_opt_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    \"\"\"\n",
    "    Handles the train and valdation loss for a single task\n",
    "    \"\"\"\n",
    "    def __init__(self, reg_param, meta_model, data, batch_size=None):\n",
    "        device = next(meta_model.parameters()).device\n",
    "\n",
    "        # stateless version of meta_model\n",
    "        self.fmodel = higher.monkeypatch(meta_model, device=device, copy_initial_weights=True)\n",
    "\n",
    "        self.n_params = len(list(meta_model.parameters()))\n",
    "        self.train_input, self.train_target, self.test_input, self.test_target = data\n",
    "        self.reg_param = reg_param\n",
    "        self.batch_size = 1 if not batch_size else batch_size\n",
    "        self.val_loss, self.val_acc = None, None\n",
    "\n",
    "    def bias_reg_f(self, bias, params):\n",
    "        # l2 biased regularization\n",
    "        return sum([((b - p) ** 2).sum() for b, p in zip(bias, params)])\n",
    "\n",
    "    def train_loss_f(self, params, hparams):\n",
    "        # biased regularized cross-entropy loss where the bias are the meta-parameters in hparams\n",
    "        out = self.fmodel(self.train_input, params=params)\n",
    "        return F.cross_entropy(out, self.train_target) + 0.5 * self.reg_param * self.bias_reg_f(hparams, params)\n",
    "\n",
    "    def val_loss_f(self, params, hparams):\n",
    "        # cross-entropy loss (uses only the task-specific weights in params\n",
    "        out = self.fmodel(self.test_input, params=params)\n",
    "        val_loss = F.cross_entropy(out, self.test_target)/self.batch_size\n",
    "        self.val_loss = val_loss.item()  # avoid memory leaks\n",
    "\n",
    "        pred = out.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        self.val_acc = pred.eq(self.test_target.view_as(pred)).sum().item() / len(self.test_target)\n",
    "\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Task at 0x1e4d2173a20>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single task set up\n",
    "task = Task(\n",
    "    reg_param, meta_model, (tr_x, tr_y, ts_x, ts_y),\n",
    "    batch_size=tr_xs.shape[0]\n",
    ")\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GradientDescent at 0x1e4d2173358>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner_opt = inner_opt_class(task.train_loss_f, **inner_opt_kwargs)\n",
    "inner_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_0=\\theta_{meta}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, List, Callable\n",
    "\n",
    "\n",
    "def inner_loop(\n",
    "    hparams: Generator[torch.Tensor, None, None],\n",
    "    params: Generator[torch.Tensor, None, None],\n",
    "    optim: GradientDescent,\n",
    "    n_steps: int,\n",
    "    log_interval: bool,\n",
    "    create_graph=False,\n",
    ") -> List[List[torch.Tensor]]:\n",
    "    params_history = [optim.get_opt_params(params)]\n",
    "    for t in range(n_steps):\n",
    "        params_history.append(optim(params_history[-1], hparams, create_graph=create_graph))\n",
    "        if log_interval and (t % log_interval == 0 or t == n_steps-1):\n",
    "            print(f't={t}, Loss: {optim.curr_loss.item():.6f}')\n",
    "    return params_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single task inner loop\n",
    "params = [\n",
    "    p.detach().clone().requires_grad_(True)\n",
    "    for p in meta_model.parameters()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0, Loss: 1.609438\n",
      "t=1, Loss: 0.613932\n",
      "t=2, Loss: 1.036275\n",
      "t=3, Loss: 0.748880\n",
      "t=4, Loss: 0.346639\n",
      "t=5, Loss: 0.142424\n",
      "t=6, Loss: 0.087797\n",
      "t=7, Loss: 0.068435\n",
      "t=8, Loss: 0.058163\n",
      "t=9, Loss: 0.051023\n",
      "t=10, Loss: 0.045587\n",
      "t=11, Loss: 0.041292\n",
      "t=12, Loss: 0.037774\n",
      "t=13, Loss: 0.034848\n",
      "t=14, Loss: 0.032355\n",
      "t=15, Loss: 0.030209\n"
     ]
    }
   ],
   "source": [
    "params_history = inner_loop(\n",
    "    meta_model.parameters(), params, inner_opt, T, log_interval=True\n",
    ")\n",
    "last_param = params_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(meta_model.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(fmodel.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single task hypergradient computation        \n",
    "# This is the approximation used in the paper CG stands for conjugate gradient\n",
    "cg_fp_map = GradientDescent(loss_f=task.train_loss_f, step_size=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the hypergradient by applying K steps of the\n",
    "# conjugate gradient method (CG).\n",
    "# It can end earlier when tol is reached\n",
    "\n",
    "params = [w.detach().requires_grad_(True) for w in last_param]\n",
    "hparams = list(meta_model.parameters())\n",
    "stochastic = False\n",
    "set_grad = True\n",
    "tol = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer_loss via task.val_loss_f\n",
    "out = fmodel(test_input, params=params)\n",
    "val_loss = F.cross_entropy(out, test_target) / batch_size\n",
    "o_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get outer gradients\n",
    "def grad_unused_zero(\n",
    "    output,\n",
    "    inputs,\n",
    "    grad_outputs=None,\n",
    "    retain_graph=False,\n",
    "    create_graph=False,\n",
    "):\n",
    "    grads = torch.autograd.grad(\n",
    "        output, inputs, grad_outputs, allow_unused=True,\n",
    "        retain_graph=retain_graph, create_graph=create_graph)\n",
    "    return tuple(\n",
    "        torch.zeros_like(v) if g is None else g\n",
    "        for g, v in zip(grads, inputs)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_outer_w = grad_unused_zero(o_loss, params, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grad_outer_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_outer_hparams = grad_unused_zero(o_loss, hparams, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_map = cg_fp_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not stochastic:\n",
    "    w_mapped = fp_map(params, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfp_map_dw(xs):\n",
    "    if stochastic:\n",
    "        w_mapped_in = fp_map(params, hparams)\n",
    "        Jfp_mapTv = torch.autograd.grad(\n",
    "            w_mapped_in, params, grad_outputs=xs, retain_graph=False)\n",
    "    else:\n",
    "        Jfp_mapTv = torch.autograd.grad(\n",
    "            w_mapped, params, grad_outputs=xs, retain_graph=True)\n",
    "    return [v - j for v, j in zip(xs, Jfp_mapTv)]\n",
    "\n",
    "\n",
    "# Conjugate Gradient\n",
    "Ax = dfp_map_dw\n",
    "b = grad_outer_w\n",
    "max_iter = 100\n",
    "epsilon = 1.0e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x\n",
    "x_last = [torch.zeros_like(bb) for bb in b]\n",
    "# Initialize residual\n",
    "r_last = [torch.zeros_like(bb).copy_(bb) for bb in b]\n",
    "# Initialize direction\n",
    "p_last = [torch.zeros_like(rr).copy_(rr) for rr in r_last]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(max_iter):\n",
    "    Ap = Ax(p_last)\n",
    "    Ap_vec = torch.cat([xx.view(-1) for xx in Ap])\n",
    "    p_last_vec = torch.cat([xx.view(-1) for xx in p_last])\n",
    "    r_last_vec = torch.cat([xx.view(-1) for xx in r_last])\n",
    "    rTr = torch.sum(r_last_vec * r_last_vec)\n",
    "    pAp = torch.sum(p_last_vec * Ap_vec)\n",
    "    # Liner Search\n",
    "    alpha = rTr / pAp\n",
    "    \n",
    "    # Update Estimation\n",
    "    x = [xx + alpha * pp for xx, pp in zip(x_last, p_last)]    \n",
    "    # Update Residual\n",
    "    r = [rr - alpha * pp for rr, pp in zip(r_last, Ap)]\n",
    "    r_vec = torch.cat([xx.view(-1) for xx in r_last])\n",
    "    \n",
    "    if float(torch.norm(r_vec)) < epsilon:\n",
    "        break\n",
    "    \n",
    "    # Update Direction\n",
    "    beta = torch.sum(r_vec * r_vec) / rTr\n",
    "    p = [rr + beta * pp for rr, pp in zip(r, p_last)]\n",
    "    \n",
    "    x_last = x\n",
    "    p_last = p\n",
    "    r_last = r\n",
    "    \n",
    "vs = x_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stochastic:\n",
    "    w_mapped = fp_map(params, hparams)\n",
    "    \n",
    "grads = torch.autograd.grad(w_mapped, hparams, grad_outputs=vs)\n",
    "grads = [g + v for g, v in zip(grads, grad_outer_hparams)]\n",
    "\n",
    "if set_grad:\n",
    "    for l, g in zip(hparams, grads):\n",
    "        if l.grad is None:\n",
    "            l.grad = torch.zeros_like(l)\n",
    "        if g is not None:\n",
    "            l.grad += g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(n_tasks, dataloader, meta_model, n_steps, get_inner_opt, reg_param, log_interval=None):\n",
    "    meta_model.train()\n",
    "    device = next(meta_model.parameters()).device\n",
    "\n",
    "    val_losses, val_accs = [], []\n",
    "    for k, batch in enumerate(dataloader):\n",
    "        tr_xs, tr_ys = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "        tst_xs, tst_ys = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "        for t_idx, (tr_x, tr_y, tst_x, tst_y) in enumerate(zip(tr_xs, tr_ys, tst_xs, tst_ys)):\n",
    "            task = Task(reg_param, meta_model, (tr_x, tr_y, tst_x, tst_y))\n",
    "            inner_opt = get_inner_opt(task.train_loss_f)\n",
    "\n",
    "            params = [p.detach().clone().requires_grad_(True) for p in meta_model.parameters()]\n",
    "            last_param = inner_loop(meta_model.parameters(), params, inner_opt, n_steps, log_interval=log_interval)[-1]\n",
    "\n",
    "            task.val_loss_f(last_param, meta_model.parameters())\n",
    "\n",
    "            val_losses.append(task.val_loss)\n",
    "            val_accs.append(task.val_acc)\n",
    "\n",
    "            if len(val_accs) >= n_tasks:\n",
    "                return np.array(val_losses), np.array(val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, batch in enumerate(dataloader):\n",
    "    start_time = time.time()\n",
    "    meta_model.train()\n",
    "\n",
    "    tr_xs, tr_ys = batch[\"train\"][0].to(device), batch[\"train\"][1].to(device)\n",
    "    tst_xs, tst_ys = batch[\"test\"][0].to(device), batch[\"test\"][1].to(device)\n",
    "\n",
    "    outer_opt.zero_grad()\n",
    "\n",
    "    val_loss, val_acc = 0, 0\n",
    "    forward_time, backward_time = 0, 0\n",
    "    for t_idx, (tr_x, tr_y, tst_x, tst_y) in enumerate(\n",
    "            zip(tr_xs, tr_ys, tst_xs, tst_ys)\n",
    "    ):\n",
    "        start_time_task = time.time()\n",
    "\n",
    "        # single task set up\n",
    "        task = Task(\n",
    "            reg_param, meta_model, (tr_x, tr_y, tst_x, tst_y), \n",
    "            batch_size=tr_xs.shape[0]\n",
    "        )\n",
    "        inner_opt = get_inner_opt(task.train_loss_f)\n",
    "\n",
    "        # single task inner loop\n",
    "        params = [\n",
    "            p.detach().clone().requires_grad_(True) \n",
    "            for p in meta_model.parameters()\n",
    "        ]\n",
    "        last_param = inner_loop(\n",
    "            meta_model.parameters(), params, inner_opt, T, \n",
    "            log_interval=inner_log_interval)[-1]\n",
    "        forward_time_task = time.time() - start_time_task\n",
    "\n",
    "        # single task hypergradient computation\n",
    "        if args.hg_mode == 'CG':\n",
    "            # This is the approximation used in the paper CG stands for conjugate gradient\n",
    "            cg_fp_map = hg.GradientDescent(loss_f=task.train_loss_f, step_size=1.)\n",
    "            hg.CG(last_param, list(meta_model.parameters()), K=K, fp_map=cg_fp_map, outer_loss=task.val_loss_f)\n",
    "        elif args.hg_mode == 'fixed_point':\n",
    "            hg.fixed_point(last_param, list(meta_model.parameters()), K=K, fp_map=inner_opt,\n",
    "                           outer_loss=task.val_loss_f)\n",
    "\n",
    "        backward_time_task = time.time() - start_time_task - forward_time_task\n",
    "\n",
    "        val_loss += task.val_loss\n",
    "        val_acc += task.val_acc/task.batch_size\n",
    "\n",
    "        forward_time += forward_time_task\n",
    "        backward_time += backward_time_task\n",
    "\n",
    "    outer_opt.step()\n",
    "    step_time = time.time() - start_time\n",
    "\n",
    "    if k % log_interval == 0:\n",
    "        print('MT k={} ({:.3f}s F: {:.3f}s, B: {:.3f}s) Val Loss: {:.2e}, Val Acc: {:.2f}.'\n",
    "              .format(k, step_time, forward_time, backward_time, val_loss, 100. * val_acc))\n",
    "\n",
    "    if k % eval_interval == 0:\n",
    "        test_losses, test_accs = evaluate(n_tasks_test, test_dataloader, meta_model, T_test, get_inner_opt,\n",
    "                                      reg_param, log_interval=inner_log_interval_test)\n",
    "\n",
    "        print(\"Test loss {:.2e} +- {:.2e}: Test acc: {:.2f} +- {:.2e} (mean +- std over {} tasks).\"\n",
    "              .format(test_losses.mean(), test_losses.std(), 100. * test_accs.mean(),\n",
    "                      100.*test_accs.std(), len(test_losses)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
