{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import higher  # tested with higher v0.2\n",
    "\n",
    "from torchmeta.datasets.helpers import omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    folder: str='data'\n",
    "    num_shots: int=5\n",
    "    num_ways: int=5\n",
    "    step_size: float=0.4\n",
    "    hidden_size: int=64\n",
    "    output_folder: str=None\n",
    "    batch_size: int=16\n",
    "    num_batches: int=100\n",
    "    num_workers: int=1\n",
    "    download: bool=True\n",
    "    use_cuda: bool=True\n",
    "    seed: int=0\n",
    "    hg_mode: str='CG'\n",
    "\n",
    "args = Args(num_shots=1, num_ways=5, num_batches=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.device = torch.device(\n",
    "    'cuda' if args.use_cuda and torch.cuda.is_available() else 'cpu')\n",
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = omniglot(\n",
    "    args.folder,\n",
    "    shots=args.num_shots,\n",
    "    ways=args.num_ways,\n",
    "    shuffle=True,\n",
    "    test_shots=15,\n",
    "    meta_train=True,\n",
    "    download=args.download\n",
    ")\n",
    "dataloader = BatchMetaDataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, **kwargs),\n",
    "        nn.BatchNorm2d(out_channels, momentum=1., track_running_stats=False),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, out_features, hidden_size=64):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            conv3x3(in_channels, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, out_features)\n",
    "\n",
    "    def forward(self, inputs, params=None):\n",
    "        features = self.features(inputs)\n",
    "        features = features.view((features.size(0), -1))\n",
    "        logits = self.classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logits, targets):\n",
    "    \"\"\"Compute the accuracy (after adaptation) of MAML on the test/query points\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : `torch.FloatTensor` instance\n",
    "        Outputs/logits of the model on the query points. This tensor has shape\n",
    "        `(num_examples, num_classes)`.\n",
    "    targets : `torch.LongTensor` instance\n",
    "        A tensor containing the targets of the query points. This tensor has \n",
    "        shape `(num_examples,)`.\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : `torch.FloatTensor` instance\n",
    "        Mean accuracy on the query points\n",
    "    \"\"\"\n",
    "    _, predictions = torch.max(logits, dim=-1)\n",
    "    return torch.mean(predictions.eq(targets).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNeuralNetwork(\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=1.0, affine=True, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvolutionalNeuralNetwork(\n",
    "    1, args.num_ways, hidden_size=args.hidden_size)\n",
    "model.to(device=args.device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 100\n",
    "eval_interval = 500\n",
    "inner_log_interval = None\n",
    "ways = 5\n",
    "inner_log_interval_test = None\n",
    "batch_size = 16\n",
    "n_tasks_test = 1000  # usually 1000 tasks are used for testing\n",
    "\n",
    "\n",
    "reg_param = 2  # reg_param = 2\n",
    "T, K = 16, 5  # T, K = 16, 5\n",
    "\n",
    "T_test = T\n",
    "inner_lr = .1\n",
    "\n",
    "cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    \"\"\"\n",
    "    Handles the train and valdation loss for a single task\n",
    "    \"\"\"\n",
    "    def __init__(self, reg_param, meta_model, data, batch_size=None):\n",
    "        device = next(meta_model.parameters()).device\n",
    "\n",
    "        # stateless version of meta_model\n",
    "        self.fmodel = higher.monkeypatch(meta_model, device=device, copy_initial_weights=True)\n",
    "\n",
    "        self.n_params = len(list(meta_model.parameters()))\n",
    "        self.train_input, self.train_target, self.test_input, self.test_target = data\n",
    "        self.reg_param = reg_param\n",
    "        self.batch_size = 1 if not batch_size else batch_size\n",
    "        self.val_loss, self.val_acc = None, None\n",
    "\n",
    "    def bias_reg_f(self, bias, params):\n",
    "        # l2 biased regularization\n",
    "        return sum([((b - p) ** 2).sum() for b, p in zip(bias, params)])\n",
    "\n",
    "    def train_loss_f(self, params, hparams):\n",
    "        # biased regularized cross-entropy loss where the bias are the meta-parameters in hparams\n",
    "        out = self.fmodel(self.train_input, params=params)\n",
    "        return F.cross_entropy(out, self.train_target) + 0.5 * self.reg_param * self.bias_reg_f(hparams, params)\n",
    "\n",
    "    def val_loss_f(self, params, hparams):\n",
    "        # cross-entropy loss (uses only the task-specific weights in params\n",
    "        out = self.fmodel(self.test_input, params=params)\n",
    "        val_loss = F.cross_entropy(out, self.test_target)/self.batch_size\n",
    "        self.val_loss = val_loss.item()  # avoid memory leaks\n",
    "\n",
    "        pred = out.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        self.val_acc = pred.eq(self.test_target.view_as(pred)).sum().item() / len(self.test_target)\n",
    "\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_step(params, loss, step_size, create_graph=True):\n",
    "    grads = torch.autograd.grad(loss, params, create_graph=create_graph)\n",
    "    return [w - step_size * g for w, g in zip(params, grads)]\n",
    "\n",
    "\n",
    "class DifferentiableOptimizer:\n",
    "    def __init__(self, loss_f, dim_mult, data_or_iter=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loss_f: callable with signature (params, hparams, [data optional]) -> loss tensor\n",
    "            data_or_iter: (x, y) or iterator over the data needed for loss_f\n",
    "        \"\"\"\n",
    "        self.data_iterator = None\n",
    "        if data_or_iter:\n",
    "            self.data_iterator = data_or_iter if hasattr(data_or_iter, '__next__') else repeat(data_or_iter)\n",
    "\n",
    "        self.loss_f = loss_f\n",
    "        self.dim_mult = dim_mult\n",
    "        self.curr_loss = None\n",
    "\n",
    "    def get_opt_params(self, params):\n",
    "        opt_params = [p for p in params]\n",
    "        opt_params.extend([torch.zeros_like(p) for p in params for _ in range(self.dim_mult-1) ])\n",
    "        return opt_params\n",
    "\n",
    "    def step(self, params, hparams, create_graph):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, params, hparams, create_graph=True):\n",
    "        with torch.enable_grad():\n",
    "            return self.step(params, hparams, create_graph)\n",
    "\n",
    "    def get_loss(self, params, hparams):\n",
    "        if self.data_iterator:\n",
    "            data = next(self.data_iterator)\n",
    "            self.curr_loss = self.loss_f(params, hparams, data)\n",
    "        else:\n",
    "            self.curr_loss = self.loss_f(params, hparams)\n",
    "        return self.curr_loss\n",
    "    \n",
    "class GradientDescent(DifferentiableOptimizer):\n",
    "\n",
    "    def __init__(self, loss_f, step_size, data_or_iter=None):\n",
    "        super(GradientDescent, self).__init__(\n",
    "            loss_f, dim_mult=1, data_or_iter=data_or_iter)\n",
    "        if callable(step_size):\n",
    "            self.step_size_f = step_size\n",
    "        else:\n",
    "            self.step_size_f = lambda x: step_size\n",
    "\n",
    "    def step(self, params, hparams, create_graph):\n",
    "        loss = self.get_loss(params, hparams)\n",
    "        sz = self.step_size_f(hparams)\n",
    "        # partial derivative per task-specific parameter phi\n",
    "        return gd_step(params, loss, sz, create_graph=create_graph)\n",
    "\n",
    "def get_inner_opt(train_loss):\n",
    "    inner_opt_class = GradientDescent\n",
    "    return inner_opt_class(train_loss, **inner_opt_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, List, Callable\n",
    "\n",
    "\n",
    "def inner_loop(\n",
    "    hparams: Generator[torch.Tensor, None, None],\n",
    "    params: Generator[torch.Tensor, None, None],\n",
    "    optim: GradientDescent,\n",
    "    n_steps: int,\n",
    "    log_interval: bool,\n",
    "    create_graph=False,\n",
    ") -> List[List[torch.Tensor]]:\n",
    "    params_history = [optim.get_opt_params(params)]\n",
    "    for t in range(n_steps):\n",
    "        params_history.append(optim(params_history[-1], hparams, create_graph=create_graph))\n",
    "        if log_interval and (t % log_interval == 0 or t == n_steps-1):\n",
    "            print(f't={t}, Loss: {optim.curr_loss.item():.6f}')\n",
    "    return params_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "with tqdm(dataloader, total=args.num_batches) as pbar:\n",
    "    for k, batch in enumerate(pbar):\n",
    "        outer_opt.zero_grad()\n",
    "        model.train()\n",
    "\n",
    "        tr_xs  = batch['train'][0].to(args.device)\n",
    "        tr_ys  = batch['train'][1].to(args.device)\n",
    "        tst_xs = batch['test'][0].to(args.device)\n",
    "        tst_ys = batch['test'][1].to(args.device)\n",
    "\n",
    "        outer_loss = torch.tensor(0., device=args.device)\n",
    "        accuracy = torch.tensor(0., device=args.device)\n",
    "\n",
    "        for t_idx, (tr_x, tr_y, tst_x, tst_y) in enumerate(\n",
    "                zip(tr_xs, tr_ys, tst_xs, tst_ys)\n",
    "        ):\n",
    "            # single task set up\n",
    "            task = Task(\n",
    "                reg_param, meta_model, (tr_x, tr_y, tst_x, tst_y), \n",
    "                batch_size=tr_xs.shape[0]\n",
    "            )\n",
    "            inner_opt = get_inner_opt(task.train_loss_f)\n",
    "            with higher.innerloop_ctx(model, inner_opt, copy_initial_weights=False) as (fmodel, diffopt):\n",
    "                tr_logit = fmodel(tr_x)\n",
    "                inner_loss = F.cross_entropy(tr_logit, tr_y)\n",
    "\n",
    "                diffopt.step(inner_loss)\n",
    "\n",
    "                tst_logit = fmodel(tst_x)\n",
    "                outer_loss += F.cross_entropy(tst_logit, tst_y)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    accuracy += get_accuracy(tst_logit, tst_y)\n",
    "\n",
    "        outer_loss.div_(args.batch_size)\n",
    "        accuracy.div_(args.batch_size)\n",
    "\n",
    "        outer_loss.backward()\n",
    "        outer_opt.step()\n",
    "\n",
    "        pbar.set_postfix(accuracy='{0:.4f}'.format(accuracy.item()))\n",
    "        if k >= args.num_batches:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
